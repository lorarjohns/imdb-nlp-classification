{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.8-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python36864bitmanningimdbvenv7d4271a15e444ba3b5da44e2b358f339",
   "display_name": "Python 3.6.8 64-bit ('manning-imdb': venv)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from random import shuffle\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nlpia.loaders import get_data\n",
    "word_vectors = get_data(name='wv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First:\n",
    "- load the dataset\n",
    "- grab the labels\n",
    "- shuffle the examples. \n",
    "\n",
    "Then:\n",
    "- tokenize it and vectorize it again using Word2vec. \n",
    "- get labels.\n",
    "- split it 80/20 into the training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(dirpath):\n",
    "    '''\n",
    "    Load positive and negative example files, combine, and shuffle.\n",
    "    '''\n",
    "    pos_path = os.path.join(dirpath, 'pos')\n",
    "    neg_path = os.path.join(dirpath, 'neg')\n",
    "    pos_label, neg_label = 1, 0\n",
    "    data = []\n",
    "\n",
    "    def label_data(polarity):\n",
    "        for filename in glob.glob(os.path.join(f\"{polarity}_path\", \"*.txt\")):\n",
    "            with open(filename, \"r\") as f:\n",
    "                data.append((f\"{polarity}_label\", f.read()))\n",
    "    label_data(\"pos\")\n",
    "    label_data(\"neg\")\n",
    "\n",
    "    shuffle(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_vectorize(data, return_expected=True):\n",
    "    '''\n",
    "    Tokenize, vectorize, and split off labels from data\n",
    "    '''\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    vectorized_data = []\n",
    "    expected = []\n",
    "\n",
    "    for sample in data:\n",
    "        tokens = tokenizer.tokenize(sample[1])\n",
    "        sample_vecs = []\n",
    "        for token in tokens:\n",
    "            try:\n",
    "                sample_vecs.append(word_vectors[token])\n",
    "            except KeyError:\n",
    "                pass\n",
    "        vectorized_data.append(sample_vecs)\n",
    "        expected.append(sample[0])\n",
    "\n",
    "    if return_expected==True:\n",
    "        return vectorized_data, expected\n",
    "    else:\n",
    "        return vectorized_data\n",
    "\n",
    "# TODO better OOV handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = preprocess(\"./train\")\n",
    "vectorized_data, expected = tokenize_and_vectorize(data)\n",
    "split = int(len(vectorized_data)*0.8)\n",
    "x_train, y_train = vectorized_data[:split], expected[:split]\n",
    "x_test, y_test = vectorized_data[split:], expected[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters \n",
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 300\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "x_train, x_test = pad_trunc(x_train,  maxlen), pad_trunc(x_test, maxlen)\n",
    "\n",
    "x_train = np.reshape(x_train, (len(x_train), maxlen, embedding_dims))\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "x_test = np.reshape(x_test, (len(x_test), maxlen, embedding_dims))\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize an empty Keras network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "\n",
    "num_neurons = 50\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras SimpleRNN:\n",
    "\n",
    "- pass each input into an RNN \n",
    "- for each token: gather the output into a vector\n",
    "\n",
    "400 = length of each input sequence\n",
    "x\n",
    "50 = number of neurons\n",
    "\n",
    "-->\n",
    "\n",
    "400 = length of output vector\n",
    "50 = length of each element in the output vector\n",
    "\n",
    "Each element in the output vector is a vector containing 50 elements, with one output per neuron, representing the network value at each time step.\n",
    "\n",
    "$[v_1[50], v_2[50], v_3[50],...,v_{400}[50]]$\n",
    "\n",
    "If return_sequences was set to False (the Keras default behavior), only a single 50-dimensional vector would be returned.\n",
    "\n",
    ">> A good rule of thumb is to try to make your model no more complex than the data you’re training on. Easier said than done, but that idea gives you a rationale for adjusting your parameters as you experiment with your dataset. A more complex model will overfit training data and not generalize well; a model that is too simple will underfit the data and also not have much interesting to say about novel data. You’ll see this discussion referred to as the bias versus variance trade-off. A model that’s overfit to the data is said to have high variance and low bias. And an underfit model is the opposite: low variance and high bias; it gets everything wrong in a consistent way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a recurrent layer\n",
    "model.add(SimpleRNN,\n",
    "            num_neurons, return_sequences=True,\n",
    "            input_shape=(maxlen, embedding_dims))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually you don't need to truncate and pad data with an RNN, which can handle variable-length inputs from time step to time step. \n",
    "If you need to pass the data into a layer that expects uniform lengths, though, you can't have outputs from the RNN that contain variable lengths.\n",
    "\n",
    "To avoid overfitting, add a dropout layer to zero out some percentage of the full sequences generated by return_sequences.\n",
    "\n",
    "Add a linear layer as a classifier: Positive sentiment/1 or Negative sentiment/0.\n",
    "This layer is a dense layer with one neuron and a sigmoid activation function.\n",
    "It expects a flat vector of $n$ float elements. \n",
    "\n",
    "A feed-forward neural network is agnostic to the order of elements, as long as you are consistent with the order. So, take the 400-element vector of 50-dimensional outputs and pass it through the `Flatten()` function, which transforms a 400x50 tensor to a vector of length 20,000. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dropout(0.2)) # zero out 20% of inputs randomly\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flattening the tensor before giving the data to the feed-forward network removes information about the order of the input.\n",
    "\n",
    "The sequential learning itself happened in the RNN layer. The classifier aggregates errors via backpropagation, encoding the relationship in the network."
   ]
  }
 ]
}